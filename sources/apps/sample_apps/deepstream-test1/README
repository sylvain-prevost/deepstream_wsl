*****************************************************************************
* SPDX-FileCopyrightText: Copyright (c) 2018-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
* SPDX-License-Identifier: LicenseRef-NvidiaProprietary
*
* NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
* property and proprietary rights in and to this material, related
* documentation and any modifications thereto. Any use, reproduction,
* disclosure or distribution of this material and related documentation
* without an express license agreement from NVIDIA CORPORATION or
* its affiliates is strictly prohibited.
*****************************************************************************

*****************************************************************************
                     deepstream-test1-app
                             README
*****************************************************************************

===============================================================================
1. Prerequisites:
===============================================================================
Please follow instructions in the apps/sample_apps/deepstream-app/README on how
to install the prerequisites for Deepstream SDK, the DeepStream SDK itself and the
apps.

You must have the following development packages installed
   GStreamer-1.0
   GStreamer-1.0 Base Plugins
   GStreamer-1.0 gstrtspserver
   X11 client-side library

To install these packages, execute the following command:
   sudo apt-get install libgstreamer-plugins-base1.0-dev libgstreamer1.0-dev \
   libgstrtspserver-1.0-dev libx11-dev

This example can be configured to use either the nvinfer or the nvinferserver
element for inference.
If nvinferserver is selected, the Triton Inference Server is used for inference
processing. In this case, the example needs to be run inside the
DeepStream-Triton docker container. Please refer
samples/configs/deepstream-app-triton/README for the steps to download the
container image and setup model repository.

===============================================================================
2. Purpose:
===============================================================================

This document shall describe about the sample deepstream-test1 application.

It is meant for demonstration of how to use the various DeepStream SDK
elements in the pipeline and extract meaningful insights from a video stream.

===============================================================================
3. To compile:
===============================================================================

  $ Set CUDA_VER in the MakeFile as per platform.
      For Jetson, CUDA_VER=11.4
      For x86, CUDA_VER=11.8
  $ sudo make (sudo not required in case of docker containers)

===============================================================================
4. Usage:
===============================================================================

  Two ways to run the application:

  1.Run with the h264 elementary stream. In this method, user needs to modify the source
    code of deepstream-test1-app to configure pipeline properties.

    $ ./deepstream-test1-app <h264_elementary_stream>

  2.Run with the yml file. In this method, user needs to update the yml file to configure
    pipeline properties.

    $ ./deepstream-test1-app <yml file>
    e.g. ./deepstream-test1-app dstest1_config.yml

NOTE: To compile the sources, run make with "sudo" or root permission.

This sample creates instance of either "nvinfer" or "nvinferserver" element
for inference. The "nvinfer" element uses TensorRT API to infer
on frames/objects. The "nvinferserver" element uses the Triton Inference
Server to infer on frames/objects. Both elements are configured through their
respective config files. Using a correct configuration for a inference element
instance is therefore very important as considerable behaviors of the instance
are parameterized through these configs.

For reference, here are the config files used for this sample :
1. The 4-class detector (referred to as pgie in this sample) uses
    dstest1_pgie_config.yml / dstest1_pgie_nvinferserver_config.yml

The GIE configuration group in the YAML configuration file of the application
can be used to set the inference plugin type (nvinfer or nvinferserver) and
corresponding plugin configuration file.

In this sample, we create one instance of either "nvinfer" or "nvinferserver",
referred as the pgie. This is a 4 class detector and it detects the classes
"Vehicle , RoadSign, TwoWheeler, Person".
The inference element attaches inference metadata to the buffer. By attaching
the probe function at the end of the pipeline, one can extract meaningful
information from this inference. Please refer the "osd_sink_pad_buffer_probe"
function in the sample code. For details on the Metadata format, refer to the
file "gstnvdsmeta.h"

